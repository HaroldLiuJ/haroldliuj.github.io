<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.1">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.1">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="IntroductionThis blog will give a simple explanation of BERT and its variants’(Mainly focus on variants) main idea and how these models improve base on its training process.">
<meta name="keywords" content="NLP,Paper Reading">
<meta property="og:type" content="article">
<meta property="og:title" content="Intention Recognization - BERT &amp; BERT Variants">
<meta property="og:url" content="http://yoursite.com/2020/09/09/bert/index.html">
<meta property="og:site_name" content="Xiao Liu&#39;s Blog">
<meta property="og:description" content="IntroductionThis blog will give a simple explanation of BERT and its variants’(Mainly focus on variants) main idea and how these models improve base on its training process.">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2020/09/09/bert/m1.png">
<meta property="og:image" content="http://yoursite.com/2020/09/09/bert/m2.png">
<meta property="og:updated_time" content="2020-09-09T09:18:53.592Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Intention Recognization - BERT &amp; BERT Variants">
<meta name="twitter:description" content="IntroductionThis blog will give a simple explanation of BERT and its variants’(Mainly focus on variants) main idea and how these models improve base on its training process.">
<meta name="twitter:image" content="http://yoursite.com/2020/09/09/bert/m1.png">
  <link rel="canonical" href="http://yoursite.com/2020/09/09/bert/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Intention Recognization - BERT & BERT Variants | Xiao Liu's Blog</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Xiao Liu's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
        
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-photos">
      
    

    <a href="/photos/" rel="section"><i class="fa fa-fw fa-image"></i>Photos</a>

  </li>
      
    
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/09/09/bert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiao Liu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiao Liu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            Intention Recognization - BERT & BERT Variants
            

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-09-09 16:55:53 / Modified: 17:18:53" itemprop="dateCreated datePublished" datetime="2020-09-09T16:55:53+08:00">2020-09-09</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/English-Articles/" itemprop="url" rel="index">
                    <span itemprop="name">English Articles</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/09/09/bert/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/09/09/bert/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This blog will give a simple explanation of BERT and its variants’(Mainly focus on variants) main idea and how these models improve base on its training process.</p><a id="more"></a>
<p>The trending improving methods are divided into 2 teams. The first team is using BERT as (part of) the encoder of other neural networks, that is another network will take BERT output as its input or concatenate BERT output and other network output as some layers’ input to make the final prediction. They often use pre-trained BERT to get some improvement.</p>
<p>The second team will focus more on the foundation of BERT. They will try to add some pre-training objectives to make BERT get more information of the corpora.</p>
<p>This blog will summarize some BERT Variants that I recently learned. I will explain the method of the original paper and show the improvement if I think the improvement is obvious.</p>
<p>If you have any questions, please contact haroldj@gmail.com. Have a nice trip!</p>
<h1 id="BERT-as-Encoder"><a href="#BERT-as-Encoder" class="headerlink" title="BERT as Encoder"></a>BERT as Encoder</h1><h3 id="1-Semantic-aware-BERT-for-Language-Understanding"><a href="#1-Semantic-aware-BERT-for-Language-Understanding" class="headerlink" title="1. Semantic-aware BERT for Language Understanding"></a>1. Semantic-aware BERT for Language Understanding</h3><p>This paper is presented by Zhuoshen Zhang et al. They hold an argument that although  BERT can learn some semantic knowledge during pertaining, it’s still not enough for BERT to generate a sentence with full semantic information. For example, in the machine reading mission, the question is <code>How many people dose the Greater Los Angeles Area have?</code> The baseline model will give the answer of <code>17.5 million</code> while the true answer is <code>over 17.5 million</code>. So the authors encoded an extra piece of semantic information and combined it with the BERT output to achieve higher performance. The architecture of the model is:</p>
<p><img src="/2020/09/09/bert/m1.png" alt></p>
<p>The left side of the model is a pre-trained BERT encoder. The authors added a Semantic Role Labeler(SRL) next to the BERT. This labeler will read the sentence and give out $m$ groups of the semantic role of each token. Each group will go through a GRU encoder. The authors concatenated the $m$ outputs and go through a fully connected layer as the semantic information. This information will concatenate with the BERT output and pass forward to finish the objective.</p>
<p>This model is having better performance on the tasks which need semantic information. But the model is too huge and the improvement is limited.</p>
<h3 id="2-Multi-Task-Deep-Neural-Networks-for-Natural-Language-Understanding"><a href="#2-Multi-Task-Deep-Neural-Networks-for-Natural-Language-Understanding" class="headerlink" title="2. Multi-Task Deep Neural Networks for Natural Language Understanding"></a>2. Multi-Task Deep Neural Networks for Natural Language Understanding</h3><p>This paper is presented by Xiaodong Liu et al and this is the current state-of-the-art model in GLUE leader board(2020.5). This model use BERT as <strong>shared context embedding encoder</strong> among several Task Specific Layers. The architecture of the model is:</p>
<p><img src="/2020/09/09/bert/m2.png" alt></p>
<p>The authors fine-tuned one BERT across 4 different NLP tasks. The objective is to approximately optimize the sum of all multi-task objectives so that the BERT can have more general knowledge of the language.</p>
<h1 id="Modifying-Pre-training-Objective"><a href="#Modifying-Pre-training-Objective" class="headerlink" title="Modifying Pre-training Objective"></a>Modifying Pre-training Objective</h1><h3 id="1-STRUCTBERT-Incorporating-Language-Structures-Into-Pre-Training-for-Deep-Language-Understanding"><a href="#1-STRUCTBERT-Incorporating-Language-Structures-Into-Pre-Training-for-Deep-Language-Understanding" class="headerlink" title="1. STRUCTBERT Incorporating Language Structures Into Pre-Training for Deep Language Understanding"></a>1. STRUCTBERT Incorporating Language Structures Into Pre-Training for Deep Language Understanding</h3><p>This paper is presented by Wei Wang et al. They are focusing the same issue as the authors’ of the Semantic-BERT, they argue that BERT does not make the most of underlying language structures. So the authors of this paper came out with 2 new pretraining objectives and got new state-of-the-art performance in a variety of downstream tasks.</p>
<p>The first new objective is <strong>Word Structural Objective</strong>. Besides Masking words in the original BERT pretraining process, the authors will also randomly shuffle K unmasked words and let the algorithm predict the right word order. Given the randomicity of token shuffling, the word objective is equivalent to maximizing the likelihood of placing every shuffled token in its correct position. More formally, this objective can be formulated as:</p>
<script type="math/tex; mode=display">
\arg \max _{\theta} \sum \log P\left(\operatorname{pos}_{1}=t_{1}, \operatorname{pos}_{2}=t_{2}, \ldots, \operatorname{pos}_{K}=t_{K} \mid t_{1}, t_{2}, \ldots, t_{K}, \theta\right)</script><p> The authors recommend to randomly shuffle 3 continuous words and shuffle 5% of trigrams are selected for random shuffling.</p>
<p>The second objective is <strong>Sentence Structural Objective</strong>. They extend the sentence prediction task by predicting both the next sentence and the previous sentence, to make the pre-trained language model aware of the sequential order of the sentences in a bidirectional manner.</p>
<p>With these two new objectives, BERT can get more semantic information and get new state-of-the-art performance on many tasks. </p>
<h3 id="2-SpanBERT"><a href="#2-SpanBERT" class="headerlink" title="2. SpanBERT"></a>2. SpanBERT</h3><p>This paper is presented by Mandar Joshi et al. The authors argue that the original BERT is limited in the problem of generating a sequence of words like MRC tasks. So they gave out a new pretraining method with a new objective: span-boundary objective. The authors firstly sampled a <strong>span length</strong> from a geometric distribution then randomly select the starting point for the span to be masked. </p>
<p>According to the paper, they masked 15% of the words in the documents as in BERT(replacing 80% of the masked tokens with [MASK], 10% with random tokens, and 10% with the original tokens). However, they performed this replacement at the span level and not for each token individually; that is, all the tokens in a span are replaced with [MASK] or sampled tokens.</p>
<p>Then they proposed a Span Boundary Objective. Let  $\mathbf{x}$  be the output of BERT and (s,e) is the start and the end position of the span. $\mathbf{p}$ is the position embedding. So the represent of the i-th masked token $\mathbf{y_i}$ is:</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\mathbf{h}_{0}=\left[\mathbf{x}_{s-1} ; \mathbf{x}_{e+1} ; \mathbf{p}_{i-s+1}\right] \\
\mathbf{h}_{1}=\text { LayerNorm }\left(\mathrm{GeLU}\left(\mathbf{W}_{1} \mathbf{h}_{0}\right)\right) \\
\mathbf{y}_{i}=\text { LayerNorm }\left(\mathrm{GeLU}\left(\mathbf{W}_{2} \mathbf{h}_{1}\right)\right)
\end{array}</script><p>They use  $\mathbf{y_i}$ to compute the probability of the token and use cross-entropy as its loss. So the loss function is the sum of Span Boundary Objective loss and regular masked language loss:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L}\left(x_{i}\right) &=\mathcal{L}_{\mathrm{MLM}}\left(x_{i}\right)+\mathcal{L}_{\mathrm{SBO}}\left(x_{i}\right) \\
&=-\log P\left(x_{i} \mid \mathbf{x}_{i}\right)-\log P\left(x_{i} \mid \mathbf{y}_{i}\right)
\end{aligned}</script><p>They also omitted the Next Sentence Prediction objective used in the original BERT since other research showed this function will hurt the performance of BERT by impeding it learning from longer full-length contexts and by adding noise for Masked Language Model objective.</p>
<p>With this pretraining method, the model has an obvious improvement on extractive QA tasks and other tasks can also benefit a little from this pretraining method</p>
<h3 id="3-RoBERTa"><a href="#3-RoBERTa" class="headerlink" title="3. RoBERTa"></a>3. RoBERTa</h3><p>This paper is proposed by Yinhan Liu et al, in which they reconstructed the whole BERT training process and find which factors can influence the final result of pretraining, then they proposed a model that summarized all the positive influences that they found call RoBERTa.</p>
<p>They typically have these findings:</p>
<ul>
<li>Using dynamic masking which will generate different patterns every time they feed a sequence to the model can achieve slightly better performance.</li>
<li>Removing Next Sentence Prediction objective just using a longer sequence that extracted from one document(no cross documents) can have better performance</li>
<li>Training on large batch size can get better performance</li>
<li>Though replace character-level representation of the input with Byte-Pair Encoding do not improve the performance even hurt a little, they believe the advantages of a universal encoding scheme outweigh the minor degradation in performance</li>
<li>They also found <strong>the data and the number of training passes through the data</strong> are two important factors, the authors said with well-designed data and passing through time, the RoBERTa will have better performance than BERT even with the same objective</li>
</ul>
<p>Future work: </p>
<ul>
<li>multi-task learning?</li>
<li>which factor is dominant? Data properties or pre-training objective   </li>
</ul>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul>
<li><a href="https://arxiv.org/pdf/1909.02209" target="_blank" rel="noopener">Zhang Z, Wu Y, Zhao H, et al. Semantics-aware bert for language understanding[J]. arXiv preprint arXiv:1909.02209, 2019.</a></li>
<li><a href="https://arxiv.org/pdf/1901.11504" target="_blank" rel="noopener">Liu X, He P, Chen W, et al. Multi-task deep neural networks for natural language understanding[J]. arXiv preprint arXiv:1901.11504, 2019.</a></li>
<li><a href="https://arxiv.org/pdf/1908.04577" target="_blank" rel="noopener">Wang W, Bi B, Yan M, et al. Structbert: Incorporating language structures into pre-training for deep language understanding[J]. arXiv preprint arXiv:1908.04577, 2019.</a></li>
<li><a href="https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00300" target="_blank" rel="noopener">Joshi M, Chen D, Liu Y, et al. Spanbert: Improving pre-training by representing and predicting spans[J]. Transactions of the Association for Computational Linguistics, 2020, 8: 64-77.</a></li>
<li><a href="https://arxiv.org/pdf/1907.11692" target="_blank" rel="noopener">Liu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining approach[J]. arXiv preprint arXiv:1907.11692, 2019.</a></li>
</ul>

    </div>

    
    
    
        
      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/NLP/" rel="tag"># NLP</a>
            
              <a href="/tags/Paper-Reading/" rel="tag"># Paper Reading</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2020/08/25/Intention-Recognization/" rel="next" title="Intention Recognization">
                  <i class="fa fa-chevron-left"></i> Intention Recognization
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
            </div>
          </div>
        
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#BERT-as-Encoder"><span class="nav-number">2.</span> <span class="nav-text">BERT as Encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Semantic-aware-BERT-for-Language-Understanding"><span class="nav-number">2.0.1.</span> <span class="nav-text">1. Semantic-aware BERT for Language Understanding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Multi-Task-Deep-Neural-Networks-for-Natural-Language-Understanding"><span class="nav-number">2.0.2.</span> <span class="nav-text">2. Multi-Task Deep Neural Networks for Natural Language Understanding</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Modifying-Pre-training-Objective"><span class="nav-number">3.</span> <span class="nav-text">Modifying Pre-training Objective</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-STRUCTBERT-Incorporating-Language-Structures-Into-Pre-Training-for-Deep-Language-Understanding"><span class="nav-number">3.0.1.</span> <span class="nav-text">1. STRUCTBERT Incorporating Language Structures Into Pre-Training for Deep Language Understanding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-SpanBERT"><span class="nav-number">3.0.2.</span> <span class="nav-text">2. SpanBERT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-RoBERTa"><span class="nav-number">3.0.3.</span> <span class="nav-text">3. RoBERTa</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#References"><span class="nav-number">3.0.4.</span> <span class="nav-text">References</span></a></li></ol></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="Xiao Liu">
  <p class="site-author-name" itemprop="name">Xiao Liu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/haroldliuj" title="GitHub &rarr; https://github.com/haroldliuj" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:haroldliuj@gmail.com" title="E-Mail &rarr; mailto:haroldliuj@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.instagram.com/haroldlogspace/" title="Instagram &rarr; https://www.instagram.com/haroldlogspace/" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiao Liu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.4.1
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script>
<script src="/js/schemes/pisces.js?v=7.4.1"></script>

<script src="/js/next-boot.js?v=7.4.1"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'FoLSl7NpGs5fSxnb59tfNULA-gzGzoHsz',
    appKey: 'Yl577c0mRRb9AIFa03rdU8c1',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn',
    path: location.pathname,
    recordIP: false,
    serverURLs: ''
  });
}, window.Valine);
</script>

</body>
</html>
