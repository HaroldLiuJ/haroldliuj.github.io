<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.1">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.1">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="Stanford CS224n Learning Notes 1: Word VectorsThis blog is just my learning notes of Stanford CS224n so I used many expressions and graphs from slides and lecture readings. If there are copyright prob">
<meta name="keywords" content="NLP,CS224n">
<meta property="og:type" content="article">
<meta property="og:title" content="CS224n Learning Notes--Word Vectors">
<meta property="og:url" content="http://yoursite.com/2019/11/20/CS224n-Learning-Notes-Word-Vectors/index.html">
<meta property="og:site_name" content="Xiao Liu&#39;s Blog">
<meta property="og:description" content="Stanford CS224n Learning Notes 1: Word VectorsThis blog is just my learning notes of Stanford CS224n so I used many expressions and graphs from slides and lecture readings. If there are copyright prob">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2019/11/20/CS224n-Learning-Notes-Word-Vectors/comatrixexp.png">
<meta property="og:image" content="http://yoursite.com/2019/11/20/CS224n-Learning-Notes-Word-Vectors/cbow.png">
<meta property="og:image" content="http://yoursite.com/2019/11/20/CS224n-Learning-Notes-Word-Vectors/skipg.png">
<meta property="og:image" content="http://yoursite.com/2019/11/20/CS224n-Learning-Notes-Word-Vectors/hsoftmax.png">
<meta property="og:updated_time" content="2019-11-24T21:47:43.326Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CS224n Learning Notes--Word Vectors">
<meta name="twitter:description" content="Stanford CS224n Learning Notes 1: Word VectorsThis blog is just my learning notes of Stanford CS224n so I used many expressions and graphs from slides and lecture readings. If there are copyright prob">
<meta name="twitter:image" content="http://yoursite.com/2019/11/20/CS224n-Learning-Notes-Word-Vectors/comatrixexp.png">
  <link rel="canonical" href="http://yoursite.com/2019/11/20/CS224n-Learning-Notes-Word-Vectors/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>CS224n Learning Notes--Word Vectors | Xiao Liu's Blog</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Xiao Liu's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
        
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-photos">
      
    

    <a href="/photos/" rel="section"><i class="fa fa-fw fa-image"></i>Photos</a>

  </li>
      
    
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/20/CS224n-Learning-Notes-Word-Vectors/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiao Liu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiao Liu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            CS224n Learning Notes--Word Vectors
            

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-11-20 15:42:23" itemprop="dateCreated datePublished" datetime="2019-11-20T15:42:23+08:00">2019-11-20</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-11-25 05:47:43" itemprop="dateModified" datetime="2019-11-25T05:47:43+08:00">2019-11-25</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/English-Articles/" itemprop="url" rel="index">
                    <span itemprop="name">English Articles</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2019/11/20/CS224n-Learning-Notes-Word-Vectors/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/11/20/CS224n-Learning-Notes-Word-Vectors/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Stanford-CS224n-Learning-Notes-1-Word-Vectors"><a href="#Stanford-CS224n-Learning-Notes-1-Word-Vectors" class="headerlink" title="Stanford CS224n Learning Notes 1: Word Vectors"></a>Stanford CS224n Learning Notes 1: Word Vectors</h1><p><strong>This blog is just my learning notes of Stanford CS224n so I used many expressions and graphs from slides and lecture readings. If there are copyright problems, please contact me. </strong></p><a id="more"></a>
<h2 id="1-Introduction-of-Word-Vectors"><a href="#1-Introduction-of-Word-Vectors" class="headerlink" title="1. Introduction of Word Vectors"></a>1. Introduction of Word Vectors</h2><p>Finally started this course at the end of November. I will update this series of learning note as well as I finished learning a unit of <a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">CS224n</a> course. Okay, let’s get started!</p>
<h3 id="1-1-WordNet"><a href="#1-1-WordNet" class="headerlink" title="1.1. WordNet"></a>1.1. WordNet</h3><p>At the very beginning, People use <strong>WordNet</strong> to represent the relationship of synonyms and hypernyms between words. However, there are some problems:</p>
<ul>
<li>This method will miss nuance.<ul>
<li>According to the slides of this lecture, “proficient” is listed as a synonym for “good” but this is true only in some context.</li>
</ul>
</li>
<li>The <strong>WordNet</strong> won’t be updated as time move on.</li>
<li>Very subjective</li>
<li>Requires human labor</li>
<li>Can’t compute accurate word similarity</li>
</ul>
<h3 id="1-2-One-hot-Vector"><a href="#1-2-One-hot-Vector" class="headerlink" title="1.2. One-hot Vector"></a>1.2. One-hot Vector</h3><p>So we should represent word with a new method. People use <strong>one-hot</strong> vectors to represent a word. The dimension of the vector is the size of vocabulary and each element represents a word. </p>
<ul>
<li>e.g.: computer = [0 0 0 0 0 1 0 0 0 0]; air = [1 0 0 0 0 0 0 0 0 0]<ul>
<li>So the sixth position is <code>computer</code> and the first element is <code>air</code></li>
</ul>
</li>
</ul>
<p>This still have some problems. We can’t know the relationship between one word and another since we just use one number to say if this “is the word” or not. So we should get someway learn to <strong>encode similarity in the vectors themselves</strong>.</p>
<h3 id="1-3-Word-Vectors"><a href="#1-3-Word-Vectors" class="headerlink" title="1.3. Word Vectors"></a>1.3. Word Vectors</h3><p>According to Distributional semantics: <strong>A word’s meaning is given by the word that frequently appear close-by.</strong> This means that if a word appears in a specific context, its synonyms will be likely to appear in the same context. So we can use context to find the similarity of words.</p>
<h2 id="2-Method-to-Find-Word-Vectors"><a href="#2-Method-to-Find-Word-Vectors" class="headerlink" title="2. Method to Find Word Vectors"></a>2. Method to Find Word Vectors</h2><h3 id="2-1-Co-occurrence-Matrix"><a href="#2-1-Co-occurrence-Matrix" class="headerlink" title="2.1. Co-occurrence Matrix"></a>2.1. Co-occurrence Matrix</h3><p>The first thing we can do is to count “what words are sitting around me”(imagine you are a word). We use a window that let us to see just a small number of words before and after the word of interest instead all the words before or after it.</p>
<p>Here’s an example from the <a href="https://web.stanford.edu/class/cs224n/readings/" target="_blank" rel="noopener">reading</a> of this course:</p>
<p><img src="/2019/11/20/CS224n-Learning-Notes-Word-Vectors/comatrixexp.png" alt></p>
<p>But this matrix is too sparse so we need to let it become denser. Here we use <a href="https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf" target="_blank" rel="noopener">SVD</a> to reduce the dimension. <strong>We use $U$ as word vector</strong>.</p>
<h4 id="Code-From-Assignment"><a href="#Code-From-Assignment" class="headerlink" title="Code(From Assignment)"></a>Code(From Assignment)</h4><h5 id="Constructing-Co-occurrence-Matrix"><a href="#Constructing-Co-occurrence-Matrix" class="headerlink" title="Constructing Co-occurrence Matrix"></a>Constructing Co-occurrence Matrix</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_co_occurrence_matrix</span><span class="params">(corpus, window_size=<span class="number">4</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Compute co-occurrence matrix for the given corpus and window_size (default of 4).</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller</span></span><br><span class="line"><span class="string">              number of co-occurring words.</span></span><br><span class="line"><span class="string">              </span></span><br><span class="line"><span class="string">              For example, if we take the document "START All that glitters is not gold END" with window size of 4,</span></span><br><span class="line"><span class="string">              "All" will co-occur with "START", "that", "glitters", "is", and "not".</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            corpus (list of list of strings): corpus of documents</span></span><br><span class="line"><span class="string">            window_size (int): size of context window</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            M (numpy matrix of shape (number of corpus words, number of corpus words)): </span></span><br><span class="line"><span class="string">                Co-occurence matrix of word counts. </span></span><br><span class="line"><span class="string">                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.</span></span><br><span class="line"><span class="string">            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    words, num_words = distinct_words(corpus)</span><br><span class="line">    M = np.zeros((num_words, num_words))</span><br><span class="line">    word2Ind = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ------------------</span></span><br><span class="line">    <span class="comment"># Write your implementation here.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(words)):</span><br><span class="line">        word2Ind[words[i]] = i</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> corpus:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(s)):</span><br><span class="line">            center_word = s[i]</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i-window_size, i + window_size + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> j &gt;= <span class="number">0</span> <span class="keyword">and</span> j != i <span class="keyword">and</span> j &lt; len(s):</span><br><span class="line">                    M[word2Ind[center_word], word2Ind[s[j]]] += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment"># ------------------</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> M, word2Ind</span><br></pre></td></tr></table></figure>
<h5 id="Perform-SVD"><a href="#Perform-SVD" class="headerlink" title="Perform SVD"></a>Perform SVD</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_to_k_dim</span><span class="params">(M, k=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)</span></span><br><span class="line"><span class="string">        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:</span></span><br><span class="line"><span class="string">            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts</span></span><br><span class="line"><span class="string">            k (int): embedding size of each word after dimension reduction</span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.</span></span><br><span class="line"><span class="string">                    In terms of the SVD from math class, this actually returns U * S</span></span><br><span class="line"><span class="string">    """</span>    </span><br><span class="line">    n_iters = <span class="number">10</span>     <span class="comment"># Use this parameter in your call to `TruncatedSVD`</span></span><br><span class="line">    M_reduced = <span class="literal">None</span></span><br><span class="line">    print(<span class="string">"Running Truncated SVD over %i words..."</span> % (M.shape[<span class="number">0</span>]))</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># ------------------</span></span><br><span class="line">        <span class="comment"># Write your implementation here.</span></span><br><span class="line">    svd = TruncatedSVD(n_components=k, n_iter=n_iters, random_state=<span class="number">42</span>)</span><br><span class="line">    svd.fit(M)</span><br><span class="line">    M_reduced = svd.components_</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># ------------------</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Done."</span>)</span><br><span class="line">    <span class="keyword">return</span> M_reduced.T</span><br></pre></td></tr></table></figure>
<h4 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h4><ul>
<li>The dimensions of the matrix change very often(new words are added very frequently and corpus changes in size)</li>
<li>The matrix is extremely sparse since most words do not co-occur</li>
<li>The matrix is very high dimensional in general</li>
<li>Quadratic cost to train(SVD)</li>
<li>Requires the incorporation of some hacks on X to account for the drastic imbalance in word frequency</li>
</ul>
<h3 id="2-2-Word2Vec"><a href="#2-2-Word2Vec" class="headerlink" title="2.2. Word2Vec"></a>2.2. Word2Vec</h3><p>Word2Vec can resolve many problems mentioned above. It comes from the training process of <em>Language Model</em> as a parameter.</p>
<h4 id="Continuous-Bage-of-Words-Model-CBOW"><a href="#Continuous-Bage-of-Words-Model-CBOW" class="headerlink" title="Continuous Bage of Words Model(CBOW)"></a>Continuous Bage of Words Model(CBOW)</h4><ul>
<li>We use this algorithm to predict a center word from the surrounding context</li>
<li>For each word, we use 2 vectors $u$(when the word is in the context) and $v$(when the word is in the center) to represent it. </li>
</ul>
<h5 id="Notation-for-CBOW-Model"><a href="#Notation-for-CBOW-Model" class="headerlink" title="Notation for CBOW Model:"></a>Notation for CBOW Model:</h5><ul>
<li><p>$X$: input(one-hot vector matrix)</p>
</li>
<li><p>$w_i:$ Word $i$ from vocabulary $V$</p>
</li>
<li><p>$\mathcal{V}\in R^{n \times|V|}$: Input word matrix</p>
</li>
<li><p>$v_i$: i-th column of $\mathcal{V}$, the input vector representation of word $w_i$</p>
</li>
<li><p>$\mathcal{U}\in R^{|V|\times n}$: Output word matrix</p>
</li>
<li><p>$u_i$: i-th row of $\mathcal{U}$, the output vector representation of word $w_i$</p>
</li>
</ul>
<h5 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h5><ol>
<li><p>We generate our one hot word vectors for the input context of size $m:(x^{c-m},…,x^{c-1},x^{c+1},…,x^{c+m})\in R^{|V|}$</p>
</li>
<li><p>We get our embedded word vectors for the context$(v_{c-m}=\mathcal{V}x^{c-m}, v_{c-m+1}=\mathcal{V}x^{c-m+1},…,v_{c+m}=\mathcal{V}x^{c+m}\in R^n)$</p>
<ul>
<li>Since $x^i$is one hot vector, so this equation just selects $v_i$ from $\mathcal{V}$</li>
</ul>
</li>
<li><p>Average these vectors to get $\hat{v}=\frac{v_{c-m}+…+v_{c+m}}{2m}\in R^n$</p>
</li>
<li><p>Generate a score vector $z=\mathcal{U}\hat{v}\in R^{|V|}$</p>
<ul>
<li>This step actually computing the similarity between each word in $V$ and the given context $\hat{v}$</li>
</ul>
</li>
<li><p>Turn the scores into probabilities $\hat{y}=softmax(z)\in R^{|V|}$</p>
<ul>
<li>$\text{softmax}(\hat{y_i}) = \frac{e^{\hat{y_i}}}{\Sigma_{k=1}^{|V|}e^{\hat{y_k}}}$</li>
</ul>
</li>
<li><p>We desire our probabilities generated, $\hat{y}\in R^{|V|}$, to match the true probabilities,$y\in R^{|V|}$, which also happens to be the one hot vector</p>
</li>
</ol>
<h5 id="Objective-function"><a href="#Objective-function" class="headerlink" title="Objective function"></a>Objective function</h5><p>We use cross-entropy as our loss function since if $\hat{y}_{true}=1$, loss = 0, if $\hat{y}_{true}\approx0$, loss is very big.( $\hat{y}_{true}$ is the position where the correct word’s one hot vector is 1)</p>
<script type="math/tex; mode=display">
H(\hat{y},y)=-\Sigma^{|V|}_{j=1}y_jlog(\hat{y_j})=-y_ilog(\hat{y_i})\text{(since $y$ is one hot)}</script><p>So the objective is:</p>
<script type="math/tex; mode=display">
\begin{align}
\text{minimize}J&=-logP(w_c|context)\\
&=-logP(u_c|\hat{v})\\
&=-log\frac{exp(u_c^T\hat{v})}{\Sigma_{j=1}^{|V|}exp(u_j^T\hat{v})}\\
&=-u_c^T\hat{v}+log\Sigma_{j=1}^{|V|}exp(u_j^T\hat{v})\\
\end{align}</script><h5 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h5><p><img src="/2019/11/20/CS224n-Learning-Notes-Word-Vectors/cbow.png" alt></p>
<h4 id="Skip-Gram-Model"><a href="#Skip-Gram-Model" class="headerlink" title="Skip-Gram Model"></a>Skip-Gram Model</h4><p>Sikp-Gram Model is a method that use a single center word as the context to predict surrounding words. Its shares the idea of CBOW but just swap $x$ and $y$ of the model.</p>
<h5 id="Steps-1"><a href="#Steps-1" class="headerlink" title="Steps"></a>Steps</h5><ol>
<li>We generate our one hot input vector $x \in R^{|V|}$ of the center word</li>
<li>We get our embedded word vector for the center word $v_c=\mathcal{V}x \in R^n$</li>
<li>Generate a score vector $z=\mathcal{U}v_c$</li>
<li>Turn the score vector into probabilities, $\hat{y}=softmax(z)$. Note that $\hat{y}_{c-m},…,\hat{y}_{c-1}, \hat{y}_{c+1},…,\hat{y}_{c+m}$ are the probabilities of observing each context word</li>
<li>We desire our probability vector generated to match the true probabilities which is $y^{c-m},…,y^{c-1},y^{c+1},…,y^{c+m}$, the one hot vectors of the actual output.</li>
</ol>
<h5 id="Objective"><a href="#Objective" class="headerlink" title="Objective"></a>Objective</h5><script type="math/tex; mode=display">
\begin{aligned} \text { minimize } J &=-\log P\left(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} | w_{c}\right) \\ &=-\log \prod_{j=0, j \neq m}^{2 m} P\left(w_{c-m+j} | w_{c}\right) \\ &=-\log \prod_{j=0, j \neq m}^{2 m} P\left(u_{c-m+j} | v_{c}\right) \\ &=-\log \prod_{j=0, j \neq m}^{2 m} \frac{\exp \left(u_{c-m+j}^{T} v_{c}\right)}{\Sigma_{k=1}^{|V|}exp \left(u_{k}^{T} v_{c}\right)} \ \\ &=-\sum_{j=0, j \neq m}^{2 m} u_{c-m+j}^{T} v_{c}+2 m \log \sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_{c}\right) \end{aligned}</script><h5 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h5><p>Only one probability vector $\hat{y}$ is computed. Skip-gram treats each context word equally: the models computes the probability for each word of appearing in the context independently of its distance to the center word.</p>
<ul>
<li>e.g.: [“I”, “like”, “NLP”]<ul>
<li>In this case if we choose <code>like</code> as center word and window size of 1. We will predict $P(u_{like+1}|like)$ and $P(u_{like-1}|like)$ where $u_{like+1}$ means the word after <code>like</code>. Then we hope $P(u_{like+1}|like)$ is <code>NLP</code>‘s one hot vector and  $P(u_{like-1}|like)$ is <code>I</code>‘s one hot vector.</li>
</ul>
</li>
</ul>
<h5 id="Graph-1"><a href="#Graph-1" class="headerlink" title="Graph"></a>Graph</h5><p><img src="/2019/11/20/CS224n-Learning-Notes-Word-Vectors/skipg.png" alt></p>
<h3 id="2-3-Speed-Up-Training"><a href="#2-3-Speed-Up-Training" class="headerlink" title="2.3. Speed Up Training"></a>2.3. Speed Up Training</h3><p>As we can see, the objective function of these two methods, there are both an add term through all the vocabulary list, which is really computational expensive. So we need so way to approximate it.</p>
<h4 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h4><p><em>Mikolov et al</em> gave a method of Negative Sampling. The idea of this method us to sample from a noise distribution$(P_n(w))$ whose probabilities match the ordering of the frequency of the vocabulary. And to train a model that can specify if the pair of data is come from training data. That mean if two words come from the training data so the are more likely have relationships while if not, they may not correlated.</p>
<p>We use $P(D=1|w,c)$ as the probability that $(w,c)$ came from the corpus data and $P(D=0|w,c)$ will be the probability that $(w,c)$ did not come from the corpus data.</p>
<script type="math/tex; mode=display">
P(D=1|w,c,\theta)=\sigma(v_c^Tv_w)=\frac{1}{1+e^{(-v_c^Tv_w)}}\\
P=(D=0|w,c,\theta) = 1 - P(D=1|w,c,\theta)</script><p>So we can bulid a objective function that tries to maximize the probability of a word and context being in the corpus data if it indeed is, and maximize the probability of a word and context not being in the corpus data if they really did not. So $\theta$ is:</p>
<script type="math/tex; mode=display">
\begin{aligned} \theta &=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in \tilde{D}} P(D=0 | w, c, \theta) \\ &=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in \tilde{D}}(1-P(D=1 | w, c, \theta)) \\ &=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log P(D=1 | w, c, \theta)+\sum_{(w, c) \in \tilde{D}} \log (1-P(D=1 | w, c, \theta)) \\ &=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \bar{D}} \log \left(1-\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right) \\ &=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right) \end{aligned}</script><p>Where $\tilde{D}$ is a false or negative corpus generated by randomly sampling.</p>
<h5 id="Objective-function-1"><a href="#Objective-function-1" class="headerlink" title="Objective function"></a>Objective function</h5><script type="math/tex; mode=display">
J=-\sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}-\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right)</script><p>For skip-gram, our new objective function for observing the context word $c-m+j$ give the center word $c$ would be </p>
<script type="math/tex; mode=display">
-\log \sigma\left(u_{c-m+j}^{T} \cdot v_{c}\right)-\sum_{k=1}^{K} \log \sigma\left(-\tilde{u}_{k}^{T} \cdot v_{c}\right)</script><p>For CBOW, the objective is:</p>
<script type="math/tex; mode=display">
-\log \sigma\left(u_{c}^{T} \cdot \hat{v}\right)-\sum_{k=1}^{K} \log \sigma\left(-\tilde{u}_{k}^{T} \cdot \hat{v}\right)</script><p>where $\hat{v}=\frac{v_{c-m}+v_{c-m+1}+\ldots+v_{c+m}}{2 m}$</p>
<p>From above we can see that the add term reduce from $|V|$ to $K$.</p>
<h5 id="Noise-Distribution"><a href="#Noise-Distribution" class="headerlink" title="Noise Distribution"></a>Noise Distribution</h5><p>The Noise Distribution is the Unigram Model raised to the power of 3/4. So that the less frequent word can have greater chance to be selected.</p>
<h4 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h4><p><em>Mikolov et al.</em> also gave hierarchical softmax as a much more efficient alternative to the normal softmax. In parctice, hierarchical softmax tends to be better for infrequent words, while negative sampling work better for frequent words and lower dimensional vectors.</p>
<p>This method represent all words in the vocabulary. Each leaf of the tree is a word, and there is a unique path from root to leaf. In this model, each node in the path from root to leaf(no include the leaf) is a vector to be learned.</p>
<p><img src="/2019/11/20/CS224n-Learning-Notes-Word-Vectors/hsoftmax.png" alt></p>
<h5 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h5><ul>
<li>$L(w)$: the number of nodes in the path from the root to the leaf $w$.</li>
<li>$n(w,i)$: the i-th node on this path with associated vector $v_{n(w,1)}$.</li>
<li>$ch(n(w,j))$: the left child of $n(w,j)$</li>
</ul>
<p>We can compute the probability as:</p>
<script type="math/tex; mode=display">
P(w|w_i)=\Pi_{j=1}^{L(w)-1}\sigma([n(w,j+1)=ch(n(w,j))]·v^T_{n(w,j)}v_{w_i} )</script><p>where:</p>
<script type="math/tex; mode=display">
[x]=\left\{\begin{array}{l}{1 \text { if } x \text { is true }} \\ {-1 \text { otherwise }}\end{array}\right.</script><p>Then we can minimize the negative log likelihood $-logP(w|w_i)$.</p>
<p>This can turn O(|V|) to O(log|V|).</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul>
<li>$CS224n: Natural \ Language \ Processing\ with\ Deep\ Learning \ Lecture\ Notes:\ Part I \ \\Word Vectors I: Introduction, SVD\ and\ Word2Vec\ Winter \ 2019$</li>
</ul>

    </div>

    
    
    
        
      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/NLP/" rel="tag"># NLP</a>
            
              <a href="/tags/CS224n/" rel="tag"># CS224n</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/10/11/用HEXO-GIT搭建博客/" rel="next" title="MacOS用HEXO+GIT搭建博客">
                  <i class="fa fa-chevron-left"></i> MacOS用HEXO+GIT搭建博客
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/12/19/Data-Science-for-Research-Psychology-Lecture-Notes-Part-1/" rel="prev" title="Data Science for Research Psychology Lecture Notes Part 1">
                  Data Science for Research Psychology Lecture Notes Part 1 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Stanford-CS224n-Learning-Notes-1-Word-Vectors"><span class="nav-number">1.</span> <span class="nav-text">Stanford CS224n Learning Notes 1: Word Vectors</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction-of-Word-Vectors"><span class="nav-number">1.1.</span> <span class="nav-text">1. Introduction of Word Vectors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-WordNet"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1. WordNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-One-hot-Vector"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2. One-hot Vector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Word-Vectors"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3. Word Vectors</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Method-to-Find-Word-Vectors"><span class="nav-number">1.2.</span> <span class="nav-text">2. Method to Find Word Vectors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Co-occurrence-Matrix"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1. Co-occurrence Matrix</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Code-From-Assignment"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">Code(From Assignment)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Constructing-Co-occurrence-Matrix"><span class="nav-number">1.2.1.1.1.</span> <span class="nav-text">Constructing Co-occurrence Matrix</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Perform-SVD"><span class="nav-number">1.2.1.1.2.</span> <span class="nav-text">Perform SVD</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Problems"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">Problems</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Word2Vec"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2. Word2Vec</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Continuous-Bage-of-Words-Model-CBOW"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">Continuous Bage of Words Model(CBOW)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Notation-for-CBOW-Model"><span class="nav-number">1.2.2.1.1.</span> <span class="nav-text">Notation for CBOW Model:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Steps"><span class="nav-number">1.2.2.1.2.</span> <span class="nav-text">Steps</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Objective-function"><span class="nav-number">1.2.2.1.3.</span> <span class="nav-text">Objective function</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Graph"><span class="nav-number">1.2.2.1.4.</span> <span class="nav-text">Graph</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Skip-Gram-Model"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">Skip-Gram Model</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Steps-1"><span class="nav-number">1.2.2.2.1.</span> <span class="nav-text">Steps</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Objective"><span class="nav-number">1.2.2.2.2.</span> <span class="nav-text">Objective</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Note"><span class="nav-number">1.2.2.2.3.</span> <span class="nav-text">Note</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Graph-1"><span class="nav-number">1.2.2.2.4.</span> <span class="nav-text">Graph</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Speed-Up-Training"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3. Speed Up Training</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Negative-Sampling"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">Negative Sampling</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Objective-function-1"><span class="nav-number">1.2.3.1.1.</span> <span class="nav-text">Objective function</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Noise-Distribution"><span class="nav-number">1.2.3.1.2.</span> <span class="nav-text">Noise Distribution</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hierarchical-Softmax"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">Hierarchical Softmax</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Notation"><span class="nav-number">1.2.3.2.1.</span> <span class="nav-text">Notation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#References"><span class="nav-number">1.2.4.</span> <span class="nav-text">References</span></a></li></ol></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.png"
      alt="Xiao Liu">
  <p class="site-author-name" itemprop="name">Xiao Liu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/haroldliuj" title="GitHub &rarr; https://github.com/haroldliuj" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:haroldliuj@gmail.com" title="E-Mail &rarr; mailto:haroldliuj@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiao Liu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.4.1
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script>
<script src="/js/schemes/pisces.js?v=7.4.1"></script>

<script src="/js/next-boot.js?v=7.4.1"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'FoLSl7NpGs5fSxnb59tfNULA-gzGzoHsz',
    appKey: 'Yl577c0mRRb9AIFa03rdU8c1',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn',
    path: location.pathname,
    recordIP: false,
    serverURLs: ''
  });
}, window.Valine);
</script>

</body>
</html>
